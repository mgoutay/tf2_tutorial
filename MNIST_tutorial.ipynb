{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First : change the runtime and hardware acceleration**\n",
    "\n",
    "Runtime $\\rightarrow$ Change runtime type\n",
    "\n",
    "- Runtime type : Python 3\n",
    "- Hardware accelerator : GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a notebook that covers the basics about Tensorflow 2.0 using an MNSIT classifier.\n",
    "You will find the corresponding PowerPoint presentation and the associated PDF (with correct fonts) on [my GitHub page](https://github.com/mgoutay/tf2_tutorial)\n",
    "\n",
    "The following are discussed :\n",
    "\n",
    "1. Introduction to Deep Learning\n",
    "   - See the slides on [GitHub](https://github.com/mgoutay/tf2_tutorial)\n",
    "2. Tensorflow for beginners\n",
    "   - What's a Tensor, a TF constant & variable\n",
    "   - Preparing a dataset\n",
    "   - Building a model with the Sequential API and training it\n",
    "3. Tensorflow for experts\n",
    "   - Creating a Tensorflow dataset\n",
    "   - Defining new layers with the Subclassing API\n",
    "   - Composing a model from custom layers\n",
    "4. Building a custom training loop\n",
    "   - Defining custom training and testing function, and a custom training loop\n",
    "   - Building a graph with *@tf.function*\n",
    "   - Adding regularization\n",
    "   - Building a custom loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "Import and initialize everyting that is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TensorFlow 2.0 on Google Collab if needed\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you selected the GPU hardware acceleration, you should at least see one GPU available :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.0.0\n",
      "Number of GPUs available : 4\n",
      "Only GPU number 0 used\n"
     ]
    }
   ],
   "source": [
    "#Set the GPU you want to use\n",
    "num_GPU = 0\n",
    "\n",
    "import tensorflow as tf\n",
    "print('Tensorflow version: ', tf.__version__)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "print('Number of GPUs available :', len(gpus))\n",
    "\n",
    "if num_GPU < len(gpus):\n",
    "    tf.config.experimental.set_visible_devices(gpus[num_GPU], 'GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpus[num_GPU], True)\n",
    "    print('Only GPU number', num_GPU, 'used')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import sys\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 80%; }\n",
       "    div#menubar-container     { width: 100%; }\n",
       "    div#maintoolbar-container { width: 100%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configure the view in Jupyter Notebook\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 80%; }\n",
    "    div#menubar-container     { width: 100%; }\n",
    "    div#maintoolbar-container { width: 100%; }\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the Powerpoint presentation or at the corresponding PDF (with correct fonts) on [my GitHub Page](https://github.com/mgoutay/tf2_tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow for beginners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow 2.0 is **very pythonic**\n",
    "\n",
    "There is a lot of equivalent functions between Numpy & Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[6 6]\n",
      " [8 8]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant([[2, 2], [2, 2]], dtype=tf.int32)\n",
    "b = tf.constant([[3, 3], [4, 4]], dtype=tf.int32)\n",
    "c = a*b\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6 6]\n",
      " [8 8]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[2, 2], [2, 2]], dtype=np.int32)\n",
    "b = np.array([[3, 3], [4, 4]], dtype=np.int32)\n",
    "c = a*b\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of a NN are created as **Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cst: tf.Tensor(\n",
      "[[2 2]\n",
      " [2 2]], shape=(2, 2), dtype=int32) \n",
      "\n",
      "var: <tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\n",
      "array([[2, 2],\n",
      "       [2, 2]], dtype=int32)> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cst = tf.constant([[2, 2], [2, 2]]) # cst is a fixed Tensor\n",
    "var = tf.Variable([[2, 2], [2, 2]]) # var will be updated during training\n",
    "\n",
    "print('cst:', cst, '\\n')\n",
    "print('var:', var, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Letâ€™s play with MNIST :\n",
    " \n",
    " MNIST is large database of handwritten digits \n",
    "\n",
    "**Goal** : predict the digit given an image\n",
    "\n",
    "![MNIST](https://github.com/mgoutay/tf2_tutorial/blob/master/Images/MNIST_all.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keras** is a high-level neural networks API\n",
    "\n",
    "It gives access to pre-made Layers\n",
    "\n",
    "![keras](https://github.com/mgoutay/tf2_tutorial/blob/master/Images/love.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step, prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset, contains 4 Numpy arrays\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Convert Numpy arrays to Tensors\n",
    "x_train = tf.convert_to_tensor(x_train, dtype=tf.float32) # [60000, 28, 28]\n",
    "y_train = tf.convert_to_tensor(y_train, dtype=tf.int32) # [60000]\n",
    "\n",
    "x_test = tf.convert_to_tensor(x_test, dtype=tf.float32) # [10000, 28, 28]\n",
    "y_test = tf.convert_to_tensor(y_test, dtype=tf.int32) # [10000]\n",
    "\n",
    "# Scale the dataset and add a channel dimension\n",
    "x_train = x_train/255.0 \n",
    "x_train = tf.expand_dims(x_train, axis=-1) # [60000, 28, 28, 1]\n",
    "\n",
    "x_test = x_test/255.0 \n",
    "x_test = tf.expand_dims(x_test, axis=-1) # [10000, 28, 28, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second step, define the model easily with the **Sequential API**\n",
    "\n",
    "![MNIST](https://github.com/mgoutay/tf2_tutorial/blob/master/Images/MNIST.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = tf.keras.models.Sequential([\n",
    "    \n",
    "    Conv2D(filters=6, kernel_size=8, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(filters=15, kernel_size=4, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax') # Outputs a probability distribution\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, compile, fit, and evaluate the model.\n",
    "\n",
    "Keras gives access to pre-made functions to help you with that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 1.2726 - accuracy: 0.6793\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 0s 7us/sample - loss: 0.3916 - accuracy: 0.8845\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 0s 7us/sample - loss: 0.2739 - accuracy: 0.9204\n",
      "10000/1 - 1s - loss: 0.1585 - accuracy: 0.9350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.21569096007943153, 0.935]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy', # Only one correct class\n",
    "              metrics=['accuracy']) # Percentage of good predictions\n",
    "\n",
    "my_model.fit(x_train, y_train, epochs=3, batch_size=1024)\n",
    "\n",
    "my_model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow for experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Tensorflow **dataset**. This helps with parallelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will create the same network ourself, using custom layers, subayers, and model.\n",
    "\n",
    "![models_equals](https://github.com/mgoutay/tf2_tutorial/blob/master/Images/model_equal.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define new layers using the **Sublclassing API**.\n",
    "\n",
    "Let's define a **linear** layer:\n",
    "![linear](https://github.com/mgoutay/tf2_tutorial/blob/master/Images/linear.JPG?raw=true)\n",
    "\n",
    "It inherits form the ``Layer`` class and needs three functions : ``__init__``, ``build``, and ``call``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class Linear(Layer):\n",
    "    \"\"\"y = Wx + b\"\"\"\n",
    "\n",
    "    def __init__(self, units=32): # Called when creating the layer\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units # units = number of neurons = output shape\n",
    "\n",
    "    def build(self, input_shape): # Called the first time the layer is used\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], self.units), \n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                               initializer='random_normal', trainable=True)\n",
    "\n",
    "    def call(self, inputs): # What the layer actually does\n",
    "        return tf.matmul(inputs, self.W) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layers are **recursively composable** with custom Layers.\n",
    "\n",
    "Let's define the **DoubleDense** layer :\n",
    "\n",
    "![DoubleDense](https://github.com/mgoutay/tf2_tutorial/blob/master/Images/double_dense.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDense(Layer):\n",
    "    \"\"\"\" Linear-relu + Linear-Softmax \"\"\"\n",
    "    \n",
    "    def __init__(self, nb_classes): # Called when creating the layer\n",
    "        super(DoubleDense, self).__init__()\n",
    "        self.nb_classes = nb_classes\n",
    "        \n",
    "    def build(self, input_shape):  # Called the first time the layer is used\n",
    "        self.linear_1 = Linear(units=128)\n",
    "        self.linear_2 = Linear(units=self.nb_classes)\n",
    "\n",
    "    def call(self, inputs): # What the layer actually does\n",
    "        x = tf.nn.relu(self.linear_1(inputs))\n",
    "        x = tf.nn.softmax(self.linear_2(x)) # Outputs a probability distribution\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also define the **ConvPool2D** Layer :\n",
    "    \n",
    "![convpool](https://github.com/mgoutay/tf2_tutorial/blob/master/Images/conv_poolJPG.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvPool2D(Layer):\n",
    "    \"\"\" Conv2D-relu + MaxPooling2D \"\"\"\n",
    "    \n",
    "    def __init__(self, nb_kernels, kernel_size): # Called at layer creation\n",
    "        super(ConvPool2D, self).__init__()\n",
    "        self.nb_kernels = nb_kernels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "    def build(self, input_shape):  # Called the first time the layer is used\n",
    "        self.conv_2D = Conv2D(filters=self.nb_kernels,\n",
    "                              kernel_size=self.kernel_size, \n",
    "                              activation='relu')\n",
    "        self.pool_2D = MaxPooling2D(pool_size=(2, 2))\n",
    "\n",
    "    def call(self, inputs): # What the layer actually does\n",
    "        x = self.conv_2D(inputs)\n",
    "        x = self.pool_2D(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, define the entire model.\n",
    "\n",
    "The class inherits from the the ``Model`` class and also needs three functions : ``__init__``, ``build``, and ``call``\n",
    "\n",
    "![model](https://github.com/mgoutay/tf2_tutorial/blob/master/Images/model.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "\n",
    "class MyModel(Model):\n",
    "    def __init__(self, nb_classes):  # Called when creating the model\n",
    "        super(MyModel, self).__init__()\n",
    "        self.nb_classes = nb_classes\n",
    "    \n",
    "    def build(self, input_shape):  # Called the first time the layer is used    \n",
    "        self.conv_pool_1 = ConvPool2D(nb_kernels=6, kernel_size=8)\n",
    "        self.conv_pool_2 = ConvPool2D(nb_kernels=15, kernel_size=4)\n",
    "        self.flatten = Flatten()\n",
    "        self.double_dense = DoubleDense(nb_classes=self.nb_classes)\n",
    "    \n",
    "    def call(self, inputs): # What the model actually does\n",
    "        self.x_0 = self.conv_pool_1(inputs)\n",
    "        self.x_1 = self.conv_pool_2(self.x_0)\n",
    "        self.x_2 = self.flatten(self.x_1)\n",
    "        self.predictions = self.double_dense(self.x_2)\n",
    "        return self.predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we difined ``x_0``,``x_1``, and ``x_2`` as attributes of the class. This way, their values are saved and can be accessed afterwards if the model is not defined in a graph. This will be shown afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create the model using pre-made functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "my_model = MyModel(nb_classes=10)\n",
    "my_model.compile(optimizer, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "59/59 [==============================] - 1s 24ms/step - loss: 1.4675\n",
      "Epoch 2/3\n",
      "59/59 [==============================] - 1s 15ms/step - loss: 0.4224\n",
      "Epoch 3/3\n",
      "59/59 [==============================] - 1s 15ms/step - loss: 0.2885\n"
     ]
    }
   ],
   "source": [
    "my_model.fit(train_ds, epochs=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_pool2d_2 (ConvPool2D)   multiple                  390       \n",
      "_________________________________________________________________\n",
      "conv_pool2d_3 (ConvPool2D)   multiple                  1455      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "double_dense_1 (DoubleDense) multiple                  18698     \n",
      "=================================================================\n",
      "Total params: 20,543\n",
      "Trainable params: 20,543\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a custom training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![t_loop](https://github.com/mgoutay/tf2_tutorial/blob/master/Images/Training_loop.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important **metrics** can be logged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Define the metrics\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **tf.GradientTape API** provides automatic differentiation. All operations executed inside a gradient tape are recorded.\n",
    "\n",
    "Let's look at a simple example:\n",
    "$$x=3$$\n",
    "$$y=x^2$$\n",
    "$$\\frac{dy}{dx} = 2x = 6$$\n",
    "\n",
    "We can easily compute that with Tensorflow :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.square(x)\n",
    "dy_dx = tape.gradient(y, x)\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can expand this example to train our model. Let's define the ``training_step`` function, wich computes one SGD step with a given batch.\n",
    "\n",
    "Simply tape all operations made during the foward path, evaluate the loss function, and let Tensorflow compute the gradients of the loss with respect to our variables (the parameters of the model).\n",
    "\n",
    "Then, use the optimizer to update the parameters.\n",
    "\n",
    "Finally, save important metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One SGD step with a given batch\n",
    "def train_step(images, labels):\n",
    "    \n",
    "    # Open a GradientTape\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        #Forward pass\n",
    "        predictions = my_model(images)\n",
    "        # Loss for this batch\n",
    "        loss = loss_function(labels, predictions)\n",
    "    \n",
    "    # Get gradients of loss w.r.t. the weights\n",
    "    gradients = tape.gradient(loss, my_model.trainable_variables)\n",
    "    # Update the weights according to our optimizer\n",
    "    optimizer.apply_gradients(zip(gradients, my_model.trainable_variables))\n",
    "    \n",
    "    # Save loss and accuracy\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define a ``test_step`` wich test the model with a test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on a given batch\n",
    "def test_step(images, labels):\n",
    "    \n",
    "    # Forward pass \n",
    "    predictions = my_model(images)\n",
    "    # Loss for this batch\n",
    "    t_loss = loss_function(labels, predictions)\n",
    "    \n",
    "    # Save loss and accuracy\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create the training loop.\n",
    "\n",
    "For each epoch:\n",
    "1. We train on the entire train dataset\n",
    "2. We test on the entire test dataset\n",
    "3. We print the saved matrics\n",
    "4. We reset every metrics before the next epoch\n",
    "5. We can measure the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,   Loss: 0.913, Accuracy: 74.123    Test Loss: 0.312, Test Accuracy: 90.760\n",
      "Epoch 2,   Loss: 0.292, Accuracy: 91.388    Test Loss: 0.222, Test Accuracy: 93.490\n",
      "Epoch 3,   Loss: 0.224, Accuracy: 93.313    Test Loss: 0.188, Test Accuracy: 94.190\n",
      "TIME =  5.386934757232666\n"
     ]
    }
   ],
   "source": [
    "my_model = MyModel(nb_classes=10)\n",
    "start = time.time()\n",
    "\n",
    "# Iterate over 3 epochs\n",
    "for epoch in range(3):\n",
    "    # Train over every batch in the training dataset\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "        \n",
    "    # Test over every batch in the testing dataset\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "    \n",
    "    # Print result\n",
    "    template = 'Epoch {:.0f},   Loss: {:.3f}, Accuracy: {:.3f}    '+ \\\n",
    "               'Test Loss: {:.3f}, Test Accuracy: {:.3f}'\n",
    "    print(template.format(epoch+1, train_loss.result(), train_accuracy.result()*100,\n",
    "                          test_loss.result(), test_accuracy.result()*100))\n",
    "\n",
    "    # Reset the metrics for the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "# Display elapsed time\n",
    "end = time.time()\n",
    "print('TIME = ', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access everything inside the model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=77327, shape=(784, 135), dtype=float32, numpy=\n",
       "array([[2.8847222 , 1.9679843 , 0.40495452, ..., 1.9004487 , 1.7545568 ,\n",
       "        1.6517036 ],\n",
       "       [2.2641244 , 2.3223224 , 3.5251288 , ..., 2.3303406 , 2.9962459 ,\n",
       "        1.3287536 ],\n",
       "       [2.4556963 , 1.8994775 , 0.48268133, ..., 1.74184   , 1.9523188 ,\n",
       "        1.540447  ],\n",
       "       ...,\n",
       "       [2.7251256 , 1.8033313 , 0.6932267 , ..., 2.6561997 , 2.166208  ,\n",
       "        3.0568416 ],\n",
       "       [1.2611175 , 1.6633322 , 0.84059316, ..., 0.83077437, 0.08721785,\n",
       "        0.92634314],\n",
       "       [3.258825  , 1.8143867 , 1.5329117 , ..., 2.9006517 , 1.9882256 ,\n",
       "        2.9892595 ]], dtype=float32)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without @tf.function\n",
    "my_model.x_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Building a **graph** to speed up training\n",
    " \n",
    " This will build the model in the old Tensorflow 1.x way: it is faster but is more like a \"black box\".\n",
    " \n",
    "You simply have to add the @tf.function decorator before the train_step and test_step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One SGD step with a given batch\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    \n",
    "    # Open a GradientTape\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        #Forward pass\n",
    "        predictions = my_model(images)\n",
    "        # Loss for this batch\n",
    "        loss = loss_function(labels, predictions)\n",
    "    \n",
    "    # Get gradients of loss w.r.t. the weights\n",
    "    gradients = tape.gradient(loss, my_model.trainable_variables)\n",
    "    # Update the weights according to our optimizer\n",
    "    optimizer.apply_gradients(zip(gradients, my_model.trainable_variables))\n",
    "    \n",
    "    # Save loss and accuracy\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on a given batch\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    \n",
    "    # Forward pass \n",
    "    predictions = my_model(images)\n",
    "    # Loss for this batch\n",
    "    t_loss = loss_function(labels, predictions)\n",
    "    \n",
    "    # Save loss and accuracy\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new model and traing it :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,   Loss: 0.947, Accuracy: 71.770    Test Loss: 0.358, Test Accuracy: 89.070\n",
      "Epoch 2,   Loss: 0.324, Accuracy: 90.337    Test Loss: 0.258, Test Accuracy: 92.220\n",
      "Epoch 3,   Loss: 0.245, Accuracy: 92.693    Test Loss: 0.204, Test Accuracy: 93.880\n",
      "TIME =  3.08809232711792\n"
     ]
    }
   ],
   "source": [
    "my_model = MyModel(nb_classes=10)\n",
    "start = time.time()\n",
    "\n",
    "# Iterate over 3 epochs\n",
    "for epoch in range(3):\n",
    "    # Train over every batch in the training dataset\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "        \n",
    "    # Test over every batch in the testing dataset\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "    \n",
    "    # Print result\n",
    "    template = 'Epoch {:.0f},   Loss: {:.3f}, Accuracy: {:.3f}    '+ \\\n",
    "               'Test Loss: {:.3f}, Test Accuracy: {:.3f}'\n",
    "    print(template.format(epoch+1, train_loss.result(), train_accuracy.result()*100,\n",
    "                          test_loss.result(), test_accuracy.result()*100))\n",
    "\n",
    "    # Reset the metrics for the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "# Display elapsed time\n",
    "end = time.time()\n",
    "print('TIME = ', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training should be much quicker!\n",
    "\n",
    "But we loose access to the value of the modelâ€™s attributes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'my_model_2/flatten/Reshape:0' shape=(784, 135) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With @tf.function\n",
    "my_model.x_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using Keras layers' parameters : see the ``kernel_regularizer`` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvPool2D(Layer):\n",
    "    \"\"\" Conv2D-relu + MaxPooling2D \"\"\"\n",
    "\n",
    "    def __init__(self, nb_kernels, kernel_size): # Called at layer creation\n",
    "        super(ConvPool2D, self).__init__()\n",
    "        self.nb_kernels = nb_kernels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "    def build(self, input_shape):  # Called the first time the layer is used\n",
    "        self.conv_2D = Conv2D(filters=self.nb_kernels,\n",
    "                              kernel_size=self.kernel_size, \n",
    "                              activation='relu',\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l=1.))\n",
    "        self.pool_2D = MaxPooling2D(pool_size=(2, 2))\n",
    "\n",
    "    def call(self, inputs): # What the layer actually does\n",
    "        x = self.conv_2D(inputs)\n",
    "        x = self.pool_2D(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using a custom layers' *loss* property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class Linear(Layer):\n",
    "    \"\"\"y = Wx + b\"\"\"\n",
    "\n",
    "    def __init__(self, units=32): # Called when creating the layer\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units # units = number of neurons = output shape\n",
    "\n",
    "    def build(self, input_shape): # Called the first time the layer is used\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], self.units), \n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                               initializer='random_normal', trainable=True)\n",
    "\n",
    "    def call(self, inputs): # What the layer actually does\n",
    "        self.l1_reg = tf.reduce_sum(tf.abs(self.W)) + tf.reduce_sum(tf.abs(self.b))\n",
    "        self.add_loss(self.l1_reg)\n",
    "        return tf.matmul(inputs, self.W) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And adding those losses to the training loop, both in the ``train_step`` and the ``test_step``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One SGD step with a given batch\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    \n",
    "    # Open a GradientTape\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        #Forward pass\n",
    "        predictions = my_model(images)\n",
    "        # Loss for this batch\n",
    "        loss = loss_function(labels, predictions)\n",
    "        # Add extra losses created during this forward pass:\n",
    "        loss += 1e-3 * sum(my_model.losses)\n",
    "    \n",
    "    # Get gradients of loss w.r.t. the weights\n",
    "    gradients = tape.gradient(loss, my_model.trainable_variables)\n",
    "    # Update the weights according to our optimizer\n",
    "    optimizer.apply_gradients(zip(gradients, my_model.trainable_variables))\n",
    "    \n",
    "    # Save loss and accuracy\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on a given batch\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    \n",
    "    # Forward pass \n",
    "    predictions = my_model(images)\n",
    "    # Loss for this batch\n",
    "    t_loss = loss_function(labels, predictions)\n",
    "    # Add extra losses created during this forward pass:\n",
    "    t_loss += 1e-3 * sum(my_model.losses)\n",
    "    \n",
    "    # Save loss and accuracy\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new model and traing it :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,   Loss: 1.412, Accuracy: 71.692    Test Loss: 0.679, Test Accuracy: 89.530\n",
      "Epoch 2,   Loss: 0.610, Accuracy: 90.717    Test Loss: 0.512, Test Accuracy: 92.820\n",
      "Epoch 3,   Loss: 0.498, Accuracy: 92.853    Test Loss: 0.436, Test Accuracy: 94.320\n",
      "TIME =  3.738415479660034\n"
     ]
    }
   ],
   "source": [
    "my_model = MyModel(nb_classes=10)\n",
    "start = time.time()\n",
    "\n",
    "# Iterate over 3 epochs\n",
    "for epoch in range(3):\n",
    "    # Train over every batch in the training dataset\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "        \n",
    "    # Test over every batch in the testing dataset\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "    \n",
    "    # Print result\n",
    "    template = 'Epoch {:.0f},   Loss: {:.3f}, Accuracy: {:.3f}    '+ \\\n",
    "               'Test Loss: {:.3f}, Test Accuracy: {:.3f}'\n",
    "    print(template.format(epoch+1, train_loss.result(), train_accuracy.result()*100,\n",
    "                          test_loss.result(), test_accuracy.result()*100))\n",
    "\n",
    "    # Reset the metrics for the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "# Display elapsed time\n",
    "end = time.time()\n",
    "print('TIME = ', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a custom loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new class, that inherits from the ``Loss`` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import Loss\n",
    "\n",
    "class CustomLoss(Loss):\n",
    "    \"\"\" Custom Sparse Cross Entropy loss with L1 regularization \"\"\"\n",
    "    \n",
    "    def __init__(self, tuning_param): # Called when creating the layer\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.tuning_param = tuning_param\n",
    "        self.SCE = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        \n",
    "    def call(self, y_true, y_pred): # What the loss function actually does\n",
    "        return self.SCE(y_true, y_pred) + self.tuning_param * sum(my_model.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_loss_function = CustomLoss(tuning_param=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the loss in the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One SGD step with a given batch\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    \n",
    "    # Open a GradientTape\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        #Forward pass\n",
    "        predictions = my_model(images)\n",
    "        # Loss for this batch\n",
    "        loss = cust_loss_function(labels, predictions)\n",
    "        \n",
    "    # Get gradients of loss w.r.t. the weights\n",
    "    gradients = tape.gradient(loss, my_model.trainable_variables)\n",
    "    # Update the weights according to our optimizer\n",
    "    optimizer.apply_gradients(zip(gradients, my_model.trainable_variables))\n",
    "    \n",
    "    # Save loss and accuracy\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on a given batch\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    \n",
    "    # Forward pass \n",
    "    predictions = my_model(images)\n",
    "    # Loss for this batch\n",
    "    t_loss = cust_loss_function(labels, predictions)\n",
    "    \n",
    "    # Save loss and accuracy\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new model and train it :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,   Loss: 1.311, Accuracy: 73.928    Test Loss: 0.675, Test Accuracy: 89.690\n",
      "Epoch 2,   Loss: 0.609, Accuracy: 90.967    Test Loss: 0.513, Test Accuracy: 93.090\n",
      "Epoch 3,   Loss: 0.500, Accuracy: 93.062    Test Loss: 0.442, Test Accuracy: 94.280\n",
      "TIME =  3.5755388736724854\n"
     ]
    }
   ],
   "source": [
    "my_model = MyModel(nb_classes=10)\n",
    "start = time.time()\n",
    "\n",
    "# Iterate over 3 epochs\n",
    "for epoch in range(3):\n",
    "    # Train over every batch in the training dataset\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "        \n",
    "    # Test over every batch in the testing dataset\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "    \n",
    "    # Print result\n",
    "    template = 'Epoch {:.0f},   Loss: {:.3f}, Accuracy: {:.3f}    '+ \\\n",
    "               'Test Loss: {:.3f}, Test Accuracy: {:.3f}'\n",
    "    print(template.format(epoch+1, train_loss.result(), train_accuracy.result()*100,\n",
    "                          test_loss.result(), test_accuracy.result()*100))\n",
    "\n",
    "    # Reset the metrics for the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "# Display elapsed time\n",
    "end = time.time()\n",
    "print('TIME = ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
