{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.0.0\n",
      "Number of GPUs available : 4\n",
      "Only GPU number 0 used\n"
     ]
    }
   ],
   "source": [
    "num_GPU = 0\n",
    "\n",
    "import tensorflow as tf\n",
    "print('Tensorflow version: ', tf.__version__)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "print('Number of GPUs available :', len(gpus))\n",
    "\n",
    "tf.config.experimental.set_visible_devices(gpus[num_GPU], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[num_GPU], True)\n",
    "print('Only GPU number', num_GPU, 'used')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import scipy\n",
    "import scipy.fftpack as fp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt  \n",
    "from matplotlib.mlab import psd\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 80%; }\n",
       "    div#menubar-container     { width: 100%; }\n",
       "    div#maintoolbar-container { width: 100%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 80%; }\n",
    "    div#menubar-container     { width: 100%; }\n",
    "    div#maintoolbar-container { width: 100%; }\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow for beginners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset, contains 4 Numpy arrays\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Convert Numpy arrays to Tensors\n",
    "x_train = tf.convert_to_tensor(x_train, dtype=tf.float32) # [60000, 28, 28]\n",
    "y_train = tf.convert_to_tensor(y_train, dtype=tf.int32) # [60000]\n",
    "\n",
    "x_test = tf.convert_to_tensor(x_test, dtype=tf.float32) # [10000, 28, 28]\n",
    "y_test = tf.convert_to_tensor(y_test, dtype=tf.int32) # [10000]\n",
    "\n",
    "# Scale the dataset and add a channel dimension\n",
    "x_train = x_train/255.0 \n",
    "x_train = tf.expand_dims(x_train, axis=-1) # [60000, 28, 28, 1]\n",
    "\n",
    "x_test = x_test/255.0 \n",
    "x_test = tf.expand_dims(x_test, axis=-1) # [10000, 28, 28, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = tf.keras.models.Sequential([\n",
    "    \n",
    "    Conv2D(filters=6, kernel_size=8, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(filters=15, kernel_size=4, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax') # Outputs a probability distribution\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 1.3336 - accuracy: 0.6400\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 0s 7us/sample - loss: 0.3372 - accuracy: 0.9027\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 0s 7us/sample - loss: 0.2316 - accuracy: 0.9322\n",
      "10000/1 - 1s - loss: 0.1376 - accuracy: 0.9435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1858606786608696, 0.9435]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy', # Only one correct class\n",
    "              metrics=['accuracy']) # Percentage of good predictions\n",
    "\n",
    "my_model.fit(x_train, y_train, epochs=3, batch_size=1024)\n",
    "\n",
    "my_model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow for experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class Linear(Layer):\n",
    "    \"\"\"y = Wx + b\"\"\"\n",
    "\n",
    "    def __init__(self, units=32): # Called when creating the layer\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units # units = number of neurons = output shape\n",
    "\n",
    "    def build(self, input_shape): # Called the first time the layer is used\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], self.units), \n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                               initializer='random_normal', trainable=True)\n",
    "\n",
    "    def call(self, inputs): # What the layer actually does\n",
    "        return tf.matmul(inputs, self.W) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDense(Layer):\n",
    "    \"\"\"\" Linear-relu + Linear-Softmax \"\"\"\n",
    "    \n",
    "    def __init__(self, nb_classes): # Called when creating the layer\n",
    "        super(DoubleDense, self).__init__()\n",
    "        self.nb_classes = nb_classes\n",
    "        \n",
    "    def build(self, input_shape):  # Called the first time the layer is used\n",
    "        self.linear_1 = Linear(units=128)\n",
    "        self.linear_2 = Linear(units=self.nb_classes)\n",
    "\n",
    "    def call(self, inputs): # What the layer actually does\n",
    "        x = tf.nn.relu(self.linear_1(inputs))\n",
    "        x = tf.nn.softmax(self.linear_2(x)) # Outputs a probability distribution\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvPool2D(Layer):\n",
    "    \"\"\" Conv2D-relu + MaxPooling2D \"\"\"\n",
    "    \n",
    "    def __init__(self, nb_kernels, kernel_size): # Called at layer creation\n",
    "        super(ConvPool2D, self).__init__()\n",
    "        self.nb_kernels = nb_kernels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "    def build(self, input_shape):  # Called the first time the layer is used\n",
    "        self.conv_2D = Conv2D(filters=self.nb_kernels,\n",
    "                              kernel_size=self.kernel_size, \n",
    "                              activation='relu')\n",
    "        self.pool_2D = MaxPooling2D(pool_size=(2, 2))\n",
    "\n",
    "    def call(self, inputs): # What the layer actually does\n",
    "        x = self.conv_2D(inputs)\n",
    "        x = self.pool_2D(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "\n",
    "class MyModel(Model):\n",
    "    def __init__(self, nb_classes):  # Called when creating the model\n",
    "        super(MyModel, self).__init__()\n",
    "        self.nb_classes = nb_classes\n",
    "    \n",
    "    def build(self, input_shape):  # Called the first time the layer is used    \n",
    "        self.conv_pool_1 = ConvPool2D(nb_kernels=6, kernel_size=8)\n",
    "        self.conv_pool_2 = ConvPool2D(nb_kernels=15, kernel_size=4)\n",
    "        self.flatten = Flatten()\n",
    "        self.double_dense = DoubleDense(nb_classes=self.nb_classes)\n",
    "    \n",
    "    def call(self, inputs): # What the model actually does\n",
    "        self.x_0 = self.conv_pool_1(inputs)\n",
    "        self.x_1 = self.conv_pool_2(self.x_0)\n",
    "        self.x_2 = self.flatten(self.x_1)\n",
    "        self.predictions = self.double_dense(self.x_2)\n",
    "        return self.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "my_model = MyModel(nb_classes=10)\n",
    "my_model.compile(optimizer, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "59/59 [==============================] - 1s 16ms/step - loss: 0.2517\n",
      "Epoch 2/3\n",
      "59/59 [==============================] - 1s 15ms/step - loss: 0.2062\n",
      "Epoch 3/3\n",
      "59/59 [==============================] - 1s 15ms/step - loss: 0.1737\n"
     ]
    }
   ],
   "source": [
    "my_model.fit(train_ds, epochs=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_pool2d (ConvPool2D)     multiple                  390       \n",
      "_________________________________________________________________\n",
      "conv_pool2d_1 (ConvPool2D)   multiple                  1455      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "double_dense (DoubleDense)   multiple                  18698     \n",
      "=================================================================\n",
      "Total params: 20,543\n",
      "Trainable params: 20,543\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Define the metrics\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.square(x)\n",
    "dy_dx = tape.gradient(y, x)\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One SGD step with a given batch\n",
    "def train_step(images, labels):\n",
    "    \n",
    "    # Open a GradientTape\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        #Forward pass\n",
    "        predictions = my_model(images)\n",
    "        # Loss for this batch\n",
    "        loss = loss_function(labels, predictions)\n",
    "    \n",
    "    # Get gradients of loss w.r.t. the weights\n",
    "    gradients = tape.gradient(loss, my_model.trainable_variables)\n",
    "    # Update the weights according to our optimizer\n",
    "    optimizer.apply_gradients(zip(gradients, my_model.trainable_variables))\n",
    "    \n",
    "    # Save loss and accuracy\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on a given batch\n",
    "def test_step(images, labels):\n",
    "    \n",
    "    # Forward pass \n",
    "    predictions = my_model(images)\n",
    "    # Loss for this batch\n",
    "    t_loss = loss_function(labels, predictions)\n",
    "    \n",
    "    # Save loss and accuracy\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,   Loss: 1.376, Accuracy: 67.325    Test Loss: 0.508, Test Accuracy: 84.890\n",
      "Epoch 2,   Loss: 0.419, Accuracy: 87.733    Test Loss: 0.325, Test Accuracy: 90.670\n",
      "Epoch 3,   Loss: 0.306, Accuracy: 91.063    Test Loss: 0.251, Test Accuracy: 92.750\n",
      "TIME =  5.570061445236206\n"
     ]
    }
   ],
   "source": [
    "my_model = MyModel(nb_classes=10)\n",
    "start = time.time()\n",
    "\n",
    "# Iterate over 3 epochs\n",
    "for epoch in range(3):\n",
    "    # Train over every batch in the training dataset\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "        \n",
    "    # Test over every batch in the testing dataset\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "    \n",
    "    # Print result\n",
    "    template = 'Epoch {:.0f},   Loss: {:.3f}, Accuracy: {:.3f}    '+ \\\n",
    "               'Test Loss: {:.3f}, Test Accuracy: {:.3f}'\n",
    "    print(template.format(epoch+1, train_loss.result(), train_accuracy.result()*100,\n",
    "                          test_loss.result(), test_accuracy.result()*100))\n",
    "\n",
    "    # Reset the metrics for the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "# Display elapsed time\n",
    "end = time.time()\n",
    "print('TIME = ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=37538, shape=(784, 135), dtype=float32, numpy=\n",
       "array([[1.6669028 , 3.6323135 , 0.        , ..., 2.73629   , 3.6377664 ,\n",
       "        1.6620849 ],\n",
       "       [0.47391742, 3.3834014 , 1.136492  , ..., 3.8753707 , 4.171783  ,\n",
       "        3.6308408 ],\n",
       "       [1.7861953 , 2.916195  , 0.        , ..., 3.0124679 , 3.9319437 ,\n",
       "        2.3233078 ],\n",
       "       ...,\n",
       "       [1.1755902 , 3.4892776 , 0.        , ..., 4.403638  , 4.303809  ,\n",
       "        2.549237  ],\n",
       "       [1.625002  , 1.8590158 , 0.        , ..., 2.4259222 , 3.6148438 ,\n",
       "        3.53547   ],\n",
       "       [1.6129595 , 4.1189694 , 0.21221375, ..., 5.450149  , 4.1386952 ,\n",
       "        5.3056626 ]], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without @tf.function\n",
    "my_model.x_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Build a **graph** to speed up training\n",
    " \n",
    " Adding the @tf.function decorator before the train_step and test_step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One SGD step with a given batch\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    \n",
    "    # Open a GradientTape\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        #Forward pass\n",
    "        predictions = my_model(images)\n",
    "        # Loss for this batch\n",
    "        loss = loss_function(labels, predictions)\n",
    "    \n",
    "    # Get gradients of loss w.r.t. the weights\n",
    "    gradients = tape.gradient(loss, my_model.trainable_variables)\n",
    "    # Update the weights according to our optimizer\n",
    "    optimizer.apply_gradients(zip(gradients, my_model.trainable_variables))\n",
    "    \n",
    "    # Save loss and accuracy\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on a given batch\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    \n",
    "    # Forward pass \n",
    "    predictions = my_model(images)\n",
    "    # Loss for this batch\n",
    "    t_loss = loss_function(labels, predictions)\n",
    "    \n",
    "    # Save loss and accuracy\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,   Loss: 0.791, Accuracy: 76.390    Test Loss: 0.297, Test Accuracy: 91.380\n",
      "Epoch 2,   Loss: 0.265, Accuracy: 91.920    Test Loss: 0.195, Test Accuracy: 93.950\n",
      "Epoch 3,   Loss: 0.186, Accuracy: 94.363    Test Loss: 0.146, Test Accuracy: 95.510\n",
      "TIME =  3.81099271774292\n"
     ]
    }
   ],
   "source": [
    "my_model = MyModel(nb_classes=10)\n",
    "start = time.time()\n",
    "\n",
    "# Iterate over 3 epochs\n",
    "for epoch in range(3):\n",
    "    # Train over every batch in the training dataset\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "        \n",
    "    # Test over every batch in the testing dataset\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "    \n",
    "    # Print result\n",
    "    template = 'Epoch {:.0f},   Loss: {:.3f}, Accuracy: {:.3f}    '+ \\\n",
    "               'Test Loss: {:.3f}, Test Accuracy: {:.3f}'\n",
    "    print(template.format(epoch+1, train_loss.result(), train_accuracy.result()*100,\n",
    "                          test_loss.result(), test_accuracy.result()*100))\n",
    "\n",
    "    # Reset the metrics for the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "# Display elapsed time\n",
    "end = time.time()\n",
    "print('TIME = ', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training should be much quicker!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'my_model_2/flatten/Reshape:0' shape=(784, 135) dtype=float32>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With @tf.function\n",
    "my_model.x_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we loose access to the value of the modelâ€™s attributes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Keras layers' parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvPool2D(Layer):\n",
    "    \"\"\" Conv2D-relu + MaxPooling2D \"\"\"\n",
    "\n",
    "    def __init__(self, nb_kernels, kernel_size): # Called at layer creation\n",
    "        super(ConvPool2D, self).__init__()\n",
    "        self.nb_kernels = nb_kernels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "    def build(self, input_shape):  # Called the first time the layer is used\n",
    "        self.conv_2D = Conv2D(filters=self.nb_kernels,\n",
    "                              kernel_size=self.kernel_size, \n",
    "                              activation='relu',\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l=1.))\n",
    "        self.pool_2D = MaxPooling2D(pool_size=(2, 2))\n",
    "\n",
    "    def call(self, inputs): # What the layer actually does\n",
    "        x = self.conv_2D(inputs)\n",
    "        x = self.pool_2D(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the layers' *loss* property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class Linear(Layer):\n",
    "    \"\"\"y = Wx + b\"\"\"\n",
    "\n",
    "    def __init__(self, units=32): # Called when creating the layer\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units # units = number of neurons = output shape\n",
    "\n",
    "    def build(self, input_shape): # Called the first time the layer is used\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], self.units), \n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                               initializer='random_normal', trainable=True)\n",
    "\n",
    "    def call(self, inputs): # What the layer actually does\n",
    "        self.l1_reg = tf.reduce_sum(tf.abs(self.W)) + tf.reduce_sum(tf.abs(self.b))\n",
    "        self.add_loss(self.l1_reg)\n",
    "        return tf.matmul(inputs, self.W) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And adding those losses to the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One SGD step with a given batch\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    \n",
    "    # Open a GradientTape\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        #Forward pass\n",
    "        predictions = my_model(images)\n",
    "        # Loss for this batch\n",
    "        loss = loss_function(labels, predictions)\n",
    "        # Add extra losses created during this forward pass:\n",
    "        loss += 1e-3 * sum(my_model.losses)\n",
    "    \n",
    "    # Get gradients of loss w.r.t. the weights\n",
    "    gradients = tape.gradient(loss, my_model.trainable_variables)\n",
    "    # Update the weights according to our optimizer\n",
    "    optimizer.apply_gradients(zip(gradients, my_model.trainable_variables))\n",
    "    \n",
    "    # Save loss and accuracy\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on a given batch\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    \n",
    "    # Forward pass \n",
    "    predictions = my_model(images)\n",
    "    # Loss for this batch\n",
    "    t_loss = loss_function(labels, predictions)\n",
    "    # Add extra losses created during this forward pass:\n",
    "    t_loss += 1e-3 * sum(my_model.losses)\n",
    "    \n",
    "    # Save loss and accuracy\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,   Loss: 1.436, Accuracy: 69.803    Test Loss: 0.726, Test Accuracy: 88.010\n",
      "Epoch 2,   Loss: 0.651, Accuracy: 89.240    Test Loss: 0.551, Test Accuracy: 91.790\n",
      "Epoch 3,   Loss: 0.540, Accuracy: 91.537    Test Loss: 0.477, Test Accuracy: 93.130\n",
      "TIME =  4.245409965515137\n"
     ]
    }
   ],
   "source": [
    "my_model = MyModel(nb_classes=10)\n",
    "start = time.time()\n",
    "\n",
    "# Iterate over 3 epochs\n",
    "for epoch in range(3):\n",
    "    # Train over every batch in the training dataset\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "        \n",
    "    # Test over every batch in the testing dataset\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "    \n",
    "    # Print result\n",
    "    template = 'Epoch {:.0f},   Loss: {:.3f}, Accuracy: {:.3f}    '+ \\\n",
    "               'Test Loss: {:.3f}, Test Accuracy: {:.3f}'\n",
    "    print(template.format(epoch+1, train_loss.result(), train_accuracy.result()*100,\n",
    "                          test_loss.result(), test_accuracy.result()*100))\n",
    "\n",
    "    # Reset the metrics for the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "# Display elapsed time\n",
    "end = time.time()\n",
    "print('TIME = ', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import Loss\n",
    "\n",
    "class CustomLoss(Loss):\n",
    "    \"\"\" Custom Sparse Cross Entropy loss with L1 regularization \"\"\"\n",
    "    \n",
    "    def __init__(self, tuning_param): # Called when creating the layer\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.tuning_param = tuning_param\n",
    "        self.SCE = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        \n",
    "    def call(self, y_true, y_pred): # What the loss function actually does\n",
    "        return self.SCE(y_true, y_pred) + self.tuning_param * sum(my_model.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_loss_function = CustomLoss(tuning_param=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One SGD step with a given batch\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    \n",
    "    # Open a GradientTape\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        #Forward pass\n",
    "        predictions = my_model(images)\n",
    "        # Loss for this batch\n",
    "        loss = cust_loss_function(labels, predictions)\n",
    "        \n",
    "    # Get gradients of loss w.r.t. the weights\n",
    "    gradients = tape.gradient(loss, my_model.trainable_variables)\n",
    "    # Update the weights according to our optimizer\n",
    "    optimizer.apply_gradients(zip(gradients, my_model.trainable_variables))\n",
    "    \n",
    "    # Save loss and accuracy\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on a given batch\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    \n",
    "    # Forward pass \n",
    "    predictions = my_model(images)\n",
    "    # Loss for this batch\n",
    "    t_loss = cust_loss_function(labels, predictions)\n",
    "    # Add extra losses created during this forward pass:\n",
    "    t_loss += 1e-3 * sum(my_model.losses)\n",
    "    \n",
    "    # Save loss and accuracy\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,   Loss: 1.351, Accuracy: 69.843    Test Loss: 0.925, Test Accuracy: 90.310\n",
      "Epoch 2,   Loss: 0.579, Accuracy: 90.955    Test Loss: 0.730, Test Accuracy: 93.110\n",
      "Epoch 3,   Loss: 0.477, Accuracy: 92.930    Test Loss: 0.652, Test Accuracy: 94.040\n",
      "TIME =  4.375347375869751\n"
     ]
    }
   ],
   "source": [
    "my_model = MyModel(nb_classes=10)\n",
    "start = time.time()\n",
    "\n",
    "# Iterate over 3 epochs\n",
    "for epoch in range(3):\n",
    "    # Train over every batch in the training dataset\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "        \n",
    "    # Test over every batch in the testing dataset\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "    \n",
    "    # Print result\n",
    "    template = 'Epoch {:.0f},   Loss: {:.3f}, Accuracy: {:.3f}    '+ \\\n",
    "               'Test Loss: {:.3f}, Test Accuracy: {:.3f}'\n",
    "    print(template.format(epoch+1, train_loss.result(), train_accuracy.result()*100,\n",
    "                          test_loss.result(), test_accuracy.result()*100))\n",
    "\n",
    "    # Reset the metrics for the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "# Display elapsed time\n",
    "end = time.time()\n",
    "print('TIME = ', end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
